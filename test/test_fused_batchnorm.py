import os
import numpy as np
import tensorflow as tf

# 1. åŠ è½½æ’ä»¶
PLUGIN_PATH = '/workspace/tensorflow_musa/build/libmusa_plugin.so'
if os.path.exists(PLUGIN_PATH):
    _ = tf.load_op_library(PLUGIN_PATH)
    print(f"âœ… æˆåŠŸåŠ è½½ MUSA æ’ä»¶: {PLUGIN_PATH}")
else:
    print(f"âŒ æ‰¾ä¸åˆ°æ’ä»¶æ–‡ä»¶ {PLUGIN_PATH}")
    exit(1)

def test_raw_ops_bn():
    print("\n" + "="*60)
    print("ğŸš€ ä½¿ç”¨ tf.raw_ops ç›´æ¥æµ‹è¯• MUSA FusedBatchNormV3 (ç»•è¿‡ Keras)")
    print("="*60)

    # 1. å‡†å¤‡æ•°æ® (å¼ºåˆ¶ float32, NHWC)
    # Shape: [Batch=2, Height=2, Width=2, Channel=4] (ç”¨å°ä¸€ç‚¹çš„æ•°æ®æ–¹ä¾¿è°ƒè¯•)
    N, H, W, C = 2, 1, 1, 32
    shape = [N, H, W, C]
    
    np.random.seed(42)
    x_val = np.random.randn(*shape).astype(np.float32)
    scale_val = np.random.rand(C).astype(np.float32)
    offset_val = np.random.rand(C).astype(np.float32)
    # è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œè¾“å…¥çš„ mean/var å³ä½¿ä¸ºç©ºä¹Ÿæ˜¯æœ‰æ„ä¹‰çš„è¾“å…¥ä½ï¼Œè¿™é‡Œç»™åˆå§‹å€¼
    mean_val = np.zeros(C).astype(np.float32) 
    var_val = np.ones(C).astype(np.float32)

    # ---------------------------------------------------------
    # 2. CPU è¿è¡ŒåŸºå‡† (Golden Result)
    # ---------------------------------------------------------
    print("æ­£åœ¨è¿è¡Œ CPU åŸºå‡†...")
    with tf.device("/CPU:0"):
        x_cpu = tf.constant(x_val)
        scale_cpu = tf.constant(scale_val)
        offset_cpu = tf.constant(offset_val)
        mean_cpu = tf.constant(mean_val)
        var_cpu = tf.constant(var_val)

        # æ˜¾å¼è°ƒç”¨ Raw Op
        # FusedBatchNormV3 è¿”å› 6 ä¸ªè¾“å‡º: [y, batch_mean, batch_var, reserve_1, reserve_2, reserve_3]
        y_cpu_raw = tf.raw_ops.FusedBatchNormV3(
            x=x_cpu,
            scale=scale_cpu,
            offset=offset_cpu,
            mean=mean_cpu, 
            variance=var_cpu,
            epsilon=0.001,
            exponential_avg_factor=1.0,
            data_format="NHWC",
            is_training=True  # ã€å…³é”®ã€‘å…ˆæµ‹ Trueï¼Œå› ä¸ºå®ƒæ˜¯æœ€éš¾çš„
        )
        y_cpu = y_cpu_raw[0] # ç¬¬ä¸€ä¸ªæ˜¯è¾“å‡ºç»“æœ
        
        # ä¸ºäº†éªŒè¯åå‘ï¼Œæˆ‘ä»¬éœ€è¦ GradientTape
        with tf.GradientTape() as tape:
            tape.watch(x_cpu)
            tape.watch(scale_cpu)
            tape.watch(offset_cpu)
            out_cpu = tf.raw_ops.FusedBatchNormV3(
                x=x_cpu, scale=scale_cpu, offset=offset_cpu, mean=mean_cpu, variance=var_cpu,
                epsilon=0.001, exponential_avg_factor=1.0, data_format="NHWC", is_training=True
            )
            loss_cpu = tf.reduce_sum(out_cpu[0])
        
        # è®¡ç®— x, scale, offset çš„æ¢¯åº¦
        grads_cpu = tape.gradient(loss_cpu, [x_cpu, scale_cpu, offset_cpu])

    # ---------------------------------------------------------
    # 3. MUSA è¿è¡Œæµ‹è¯• (Target Result)
    # ---------------------------------------------------------
    print("æ­£åœ¨è¿è¡Œ MUSA æµ‹è¯•...")
    try:
        with tf.device("/device:MUSA:0"):
            x_musa = tf.constant(x_val)
            scale_musa = tf.constant(scale_val)
            offset_musa = tf.constant(offset_val)
            mean_musa = tf.constant(mean_val) # dummy for training=True
            var_musa = tf.constant(var_val)   # dummy for training=True

            # ã€é‡ç‚¹ã€‘ï¼šè¿™é‡Œä¼šå¼ºåˆ¶ TensorFlow æŸ¥æ‰¾æ³¨å†Œåœ¨ MUSA ä¸Šçš„ FusedBatchNormV3 Kernel
            # æ­¤æ—¶ä½ çš„ C++ ä»£ç é‡Œçš„ ">>>>> [DEBUG]..." å¿…é¡»å‡ºç°ï¼
            y_musa_raw = tf.raw_ops.FusedBatchNormV3(
                x=x_musa,
                scale=scale_musa,
                offset=offset_musa,
                mean=mean_musa,
                variance=var_musa,
                epsilon=0.001,
                exponential_avg_factor=1.0,
                data_format="NHWC",
                is_training=True
            )
            y_musa = y_musa_raw[0]

            # åå‘æµ‹è¯•
            with tf.GradientTape() as tape:
                tape.watch(x_musa)
                tape.watch(scale_musa)
                tape.watch(offset_musa)
                out_musa = tf.raw_ops.FusedBatchNormV3(
                    x=x_musa, scale=scale_musa, offset=offset_musa, mean=mean_musa, variance=var_musa,
                    epsilon=0.001, exponential_avg_factor=1.0, data_format="NHWC", is_training=True
                )
                loss_musa = tf.reduce_sum(out_musa[0])
            
            grads_musa = tape.gradient(loss_musa, [x_musa, scale_musa, offset_musa])

    except Exception as e:
        print(f"\nâŒ MUSA è¿è¡Œå´©æºƒ: {e}")
        return

    # ---------------------------------------------------------
    # 4. ç»“æœæ¯”å¯¹
    # ---------------------------------------------------------
    print("\n" + "-"*30)
    print("ğŸ“Š ç²¾åº¦æ¯”å¯¹ç»“æœ")
    print("-" * 30)

    # å‰å‘ Y
    diff_y = np.abs(y_cpu.numpy() - y_musa.numpy()).max()
    print(f"Forward Output Diff (Y) : {diff_y:.6e}")

    # åå‘ DX
    diff_dx = np.abs(grads_cpu[0].numpy() - grads_musa[0].numpy()).max()
    print(f"Backward Grad Diff (dX): {diff_dx:.6e}")
    
    # åå‘ dScale
    diff_dscale = np.abs(grads_cpu[1].numpy() - grads_musa[1].numpy()).max()
    print(f"Backward Grad Diff (dS): {diff_dscale:.6e}")

    # åå‘ dOffset
    diff_doffset = np.abs(grads_cpu[2].numpy() - grads_musa[2].numpy()).max()
    print(f"Backward Grad Diff (dB): {diff_doffset:.6e}")

    if diff_y < 1e-4 and diff_dx < 1e-4:
        print("\nâœ… [SUCCESS] ç®—å­ç²¾åº¦éªŒè¯é€šè¿‡ï¼")
    else:
        print("\nâŒ [FAIL] ç²¾åº¦è¯¯å·®è¿‡å¤§ï¼")

if __name__ == "__main__":
    test_raw_ops_bn()
